{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset using Pandas\n",
    "df = pd.read_csv(\"LBW_Dataset.csv\")\n",
    "\n",
    "# Normalization & Scaling Functions using Numpy & Pandas\n",
    "\n",
    "# Outlier Scaling using .quantile() Pandas methods\n",
    "def scale_outlier(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    min_bound = Q1 - 1.5*IQR\n",
    "    max_bound = Q3 + 1.5*IQR\n",
    "    df[column] = np.where(df[column] > max_bound, max_bound, df[column])\n",
    "    df[column] = np.where(df[column] < min_bound, min_bound, df[column])\n",
    "\n",
    "# Min-Max Scaling using .min() and .max() Pandas methods\n",
    "def min_max_scaling(df):    \n",
    "    df_norm = df.copy()\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())        \n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Drop the columns Delivery Phase(1: 90, 2: 2, NaN: 4) and Education(5: 93, NaN: 3)\n",
    "df = df.drop([\"Delivery phase\", \"Education\", \"Community\"], axis = 1)\n",
    "\n",
    "# Replacing Nan of Weights with the Mean of its respective Result category\n",
    "mean_0 = (df.loc[df['Result'] == 0])['Weight'].mean()\n",
    "mean_1 = (df.loc[df['Result'] == 1])['Weight'].mean()\n",
    "\n",
    "df[\"Weight\"] = np.where((df[\"Result\"] == 0) & (df[\"Weight\"].isna()), mean_0, df[\"Weight\"])\n",
    "df[\"Weight\"] = np.where((df[\"Result\"] == 1) & (df[\"Weight\"].isna()), mean_1, df[\"Weight\"])\n",
    "\n",
    "# For now, Filling Numeric Columned NaN Values with Mean\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
    "df[\"HB\"] = df[\"HB\"].fillna(df[\"HB\"].mean())\n",
    "df[\"BP\"] = df[\"BP\"].fillna(df[\"BP\"].mean())\n",
    "\n",
    "# Very Basic Method of taking care of Outliers(Replace with IQR, Min-Max) for Age & BP columns\n",
    "scale_outlier(df, \"Age\")\n",
    "scale_outlier(df, \"BP\")\n",
    "\n",
    "# Labelling Residence = 2 as Residence = 0 to get Binary Labelled Column (Before: Residence(1,2), After: Residence(1,0))\n",
    "df[\"Residence\"] = np.where(df[\"Residence\"] == 2, 0, df[\"Residence\"])\n",
    "# Filling NaN with Mode = 1\n",
    "df[\"Residence\"] = df[\"Residence\"].fillna(1)\n",
    "\n",
    "# Converting IFA(int) to IFA(float)\n",
    "df[\"IFA\"] = df[\"IFA\"].astype(float)\n",
    "\n",
    "# Moving converted Float Result, to get it as the last Column\n",
    "res = df[\"Result\"].astype(float)\n",
    "df = df.drop([\"Result\"], axis = 1)\n",
    "df[\"Result\"] = res\n",
    "\n",
    "# Performing Normalization of the dataset (into ranges from 0 to 1) using Pandas\n",
    "df = min_max_scaling(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train-Test Splits of the dataset using .train_test_split() in Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1:].values\n",
    "\n",
    "X = np.insert(X, 0, np.ones(np.shape(X)[0]), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return [1 / (1 + math.exp(-ele)) for ele in x ]\n",
    "\n",
    "class layer():\n",
    "    def __init__(self, input_units, output_units, alpha = 0.00001, activation = 'tanh'):\n",
    "        \n",
    "        self.input_units = input_units\n",
    "        self.output_units = output_units\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale = np.sqrt(2/(input_units+output_units)), \n",
    "                                        size = (input_units,output_units))\n",
    "        self.input = np.zeros(output_units)\n",
    "        self.activated_output = np.zeros(output_units)\n",
    "        self.forward_units = np.zeros(output_units)\n",
    "        \n",
    "        # adam optimiser : parameters\n",
    "        self.alpha = alpha\n",
    "        self.t = 0\n",
    "        self.m = 0\n",
    "        self.v = 0;\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.99\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "    def forward_prop(self, inputs):\n",
    "        forward_units = np.dot(inputs, self.weights)\n",
    "        self.input = inputs.reshape(self.input_units)\n",
    "        self.forward_units = forward_units.reshape(self.output_units)\n",
    "        \n",
    "        if self.activation == 'tanh':\n",
    "            self.activated_output = np.tanh(forward_units)\n",
    "            \n",
    "        elif self.activation == 'relu':\n",
    "            self.activated_output = np.maximum(0, forward_units)\n",
    "            \n",
    "        elif self.activation == 'logistic':\n",
    "            self.activated_output = sigmoid(forward_units)\n",
    "            \n",
    "        elif self.activation == 'identity':\n",
    "            self.activated_output = forward_units\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def update_weights(self, grad):\n",
    "        # adam optimiser\n",
    "        grad = grad.reshape(self.weights.shape)\n",
    "        self.m = self.beta_1*self.m + grad*(1-self.beta_1)\n",
    "        self.v = self.beta_2*self.v + np.square(grad)*(1-self.beta_2)\n",
    "        self.t += 1\n",
    "        \n",
    "        m_hat = self.m/(1 - pow(self.beta_1, self.t))\n",
    "        v_hat = self.v/(1 - pow(self.beta_2, self.t))\n",
    "        tmp = self.alpha*(m_hat/(np.sqrt(v_hat) + self.epsilon))\n",
    "                          \n",
    "        self.weights = self.weights - tmp          \n",
    "    \n",
    "    def loss_function(self, target):\n",
    "        return [-(target*math.log(ele) + (1-target)*math.log(1-ele)) for ele in self.activated_output]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_sigmoid(z, A, y):\n",
    "    grad = (z-y)*A\n",
    "    return grad\n",
    "\n",
    "def gradient_tanh(z, y, w, A, i):\n",
    "    w = np.array((z-y) * w)\n",
    "    A_square = 1 - np.square(A)\n",
    "    A_square = A_square.reshape(np.shape(A)[0],1)\n",
    "    A = w * A_square    \n",
    "    grad = A*i\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_units = 6+1 # 1st col for the bias term + 6 parameter\n",
    "output_units = 1\n",
    "hidden_layer1_units = 20\n",
    "\n",
    "classifier = []\n",
    "classifier.append(layer(input_units, hidden_layer1_units))\n",
    "classifier.append(layer(hidden_layer1_units, output_units, activation = 'logistic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_iters = 40000\n",
    "for i in range(num_iters):\n",
    "    for ind, inputs in enumerate(X_train):\n",
    "        outputs = y_train[ind]\n",
    "        for layer in classifier:\n",
    "            inputs = layer.forward_prop(inputs)\n",
    "\n",
    "        prediction = inputs  \n",
    "        \n",
    "        hidden_layer = classifier[0]\n",
    "        grad = gradient_tanh(prediction, outputs, output_layer.weights, hidden_layer.activated_output, X_train[ind])\n",
    "        hidden_layer.update_weights(grad) \n",
    "        \n",
    "        output_layer = classifier[1]\n",
    "        grad = gradient_sigmoid(prediction, output_layer.input, outputs)    \n",
    "        output_layer.update_weights(grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, y, network):\n",
    "    predicted = []\n",
    "    for ind, inputs in enumerate(x):\n",
    "        outputs = y[ind]\n",
    "        for layer in network:\n",
    "            inputs = layer.forward_prop(inputs)\n",
    "        \n",
    "        prediction = inputs\n",
    "        if prediction[0] > 0.5:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "            \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CM(y_test, y_test_obs):\n",
    "\n",
    "    cm=[[0,0],[0,0]]\n",
    "    fp=0\n",
    "    fn=0\n",
    "    tp=0\n",
    "    tn=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "            tp=tp+1\n",
    "        if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "            tn=tn+1\n",
    "        if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "            fp=fp+1\n",
    "        if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "            fn=fn+1\n",
    "            \n",
    "    cm[0][0]=tn\n",
    "    cm[0][1]=fp\n",
    "    cm[1][0]=fn\n",
    "    cm[1][1]=tp\n",
    "\n",
    "    p= tp/(tp+fp)\n",
    "    r=tp/(tp+fn)\n",
    "    f1=(2*p*r)/(p+r)\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "\n",
    "    print(\"Confusion Matrix : \")\n",
    "    print(cm)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Precision : {p}\")\n",
    "    print(f\"Recall : {r}\")\n",
    "    print(f\"F1 SCORE : {f1}\")\n",
    "    print(f\"Accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Test dataset\")\n",
    "t = predict(X_test, y_test, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[4, 2], [4, 19]]\n",
      "\n",
      "\n",
      "Precision : 0.9047619047619048\n",
      "Recall : 0.8260869565217391\n",
      "F1 SCORE : 0.8636363636363636\n",
      "Accuracy : 0.7931034482758621\n"
     ]
    }
   ],
   "source": [
    "CM(y_test, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
